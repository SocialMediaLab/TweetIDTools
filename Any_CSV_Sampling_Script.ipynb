{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SocialMediaLab/Tweets_Sampling_Toolkit/blob/main/Any_CSV_Sampling_Script.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Documentation: https://docs.google.com/document/d/165K_EQBI1VquJHaJtxN97qFhNg_0EJ7Fjp4DIDHNHcI/edit **"
      ],
      "metadata": {
        "id": "ONBK6BMfQ5tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount your google drive"
      ],
      "metadata": {
        "id": "ldzVtPpFpXVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zgdlRPt81Y-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Sampling Function"
      ],
      "metadata": {
        "id": "wffZyecQpeqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import math\n",
        "import subprocess\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import zipfile\n",
        "\n",
        "# Check if file is of .csv type\n",
        "def check_csv(input_file_path):\n",
        "  file_type = input_file_path.split(\".\")[-1]\n",
        "  return file_type == 'csv'\n",
        "\n",
        "# Check if file is of .zip type\n",
        "def check_zip(input_file_path):\n",
        "  file_type = input_file_path.split(\".\")[-1]\n",
        "  return file_type == 'zip'\n",
        "\n",
        "# Unzip zip files\n",
        "def unzip(input_file_path, output_dir):\n",
        "\n",
        "  print(f'Unzipping {input_file_path} to {output_dir}')\n",
        "\n",
        "  if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "  try:\n",
        "      with zipfile.ZipFile(input_file_path, 'r') as zip_ref:\n",
        "          zip_ref.extractall(output_dir)\n",
        "          extracted_files = zip_ref.namelist()\n",
        "  except zipfile.BadZipFile:\n",
        "      print(f\"Error: The file {input_file_path} is not a valid zip file.\")\n",
        "      return []\n",
        "\n",
        "  out_files = [os.path.join(output_dir, e) for e in extracted_files]\n",
        "\n",
        "  print(f'Done unzipping {input_file_path} to {output_dir}')\n",
        "\n",
        "  return out_files\n",
        "\n",
        "def get_chunk_counts(input_file_path, chunksize):\n",
        "  with open(input_file_path, 'r') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    row_count = sum(1 for _ in reader) - 1\n",
        "\n",
        "  num_chunks = math.ceil(row_count / chunksize)\n",
        "  return num_chunks\n",
        "\n",
        "# Chunk a file\n",
        "def chunk_file(input_file_path, sample_percent=0.01, chunksize=10000):\n",
        "  # Step 1: Get the number of rows\n",
        "  with open(input_file_path, 'r') as fp:\n",
        "    reader = csv.reader(fp)\n",
        "    row_count = sum(1 for _ in reader) - 1\n",
        "\n",
        "  # Step 2: Check if file is small enough to read in one go\n",
        "  if (chunksize >= row_count and row_count <= 50000):\n",
        "    return pd.read_csv(input_file_path).sample(frac=sample_percent)\n",
        "\n",
        "  # Step 3: Determine the number of chunks\n",
        "  num_chunks = math.ceil(row_count / chunksize)\n",
        "  nrows = [chunksize]*(num_chunks)\n",
        "  nrows[-1] = row_count - (chunksize*(num_chunks - 1))\n",
        "\n",
        "  # Step 4: Iterate through each line of the file\n",
        "  with tqdm(total=num_chunks, desc=\"Processing CSV\", unit=\"chunk\") as pbar:\n",
        "    with open(input_file_path, 'r') as fp:\n",
        "\n",
        "      # Get the header\n",
        "      reader = csv.reader(fp)\n",
        "      header = next(reader)\n",
        "      num_col = len(header)\n",
        "\n",
        "      # Counter for tracking row in chunks and skipped lines\n",
        "      counts = 0\n",
        "      skipped = 0\n",
        "\n",
        "      chunk_data = []\n",
        "      samples = []\n",
        "\n",
        "      # Iterate through each line\n",
        "      for row in reader:\n",
        "        row_len = len(row)\n",
        "\n",
        "        if row_len > num_col:\n",
        "          skipped += 1\n",
        "          continue\n",
        "        elif row_len < num_col:\n",
        "          skipped += 1\n",
        "          continue\n",
        "        else:\n",
        "          chunk_data.append(row)\n",
        "\n",
        "        counts += 1\n",
        "\n",
        "        # Check if count matches the current number of rows to be read with respect to a chunk\n",
        "        #   - If yes, then sample the data and append it to the samples array\n",
        "        if counts == nrows[0]:\n",
        "          nrows.pop(0)\n",
        "          temp = pd.DataFrame(chunk_data, columns=header)\n",
        "          temp_samples = temp.sample(frac=sample_percent)\n",
        "          samples.append(temp_samples)\n",
        "          chunk_data = []\n",
        "          counts = 0\n",
        "          pbar.update()\n",
        "\n",
        "  # Step 5: Create a dataframe containing all samples and return\n",
        "  sample_df = pd.concat(samples)\n",
        "  print(f'Expected Samples: {row_count * sample_percent}, Actual Samples: {sample_df.shape[0]}, Skipped: {skipped}')\n",
        "\n",
        "  return row_count, sample_df\n",
        "\n",
        "# Better and faster\n",
        "def chunk_file(input_file_path, sample_percent, chunksize):\n",
        "\n",
        "  samples = []\n",
        "  total_rows = 0\n",
        "  chunks = pd.read_csv(input_file_path, chunksize=chunksize)\n",
        "  num_chunks = get_chunk_counts(input_file_path, chunksize)\n",
        "\n",
        "  with tqdm(total=num_chunks, desc=\"Processing CSV\", unit=\"chunk\", leave=True) as pbar:\n",
        "    for chunk in chunks:\n",
        "      samples.append(chunk.sample(frac=sample_percent))\n",
        "      total_rows += chunk.shape[0]\n",
        "      pbar.update(1)\n",
        "\n",
        "  sample_df = pd.concat(samples)\n",
        "\n",
        "  return total_rows, sample_df\n",
        "\n",
        "# Sampling file code\n",
        "def random_sampler(input_file_path, output_file_path, sample_percent=0.01, chunksize=10000):\n",
        "  \"\"\"\n",
        "  # Purpose:\n",
        "    * Sample a dataset and save the sampled dataset to another file\n",
        "  # Args:\n",
        "    * input_file_path (str): path to the file to be sampled\n",
        "    * output_file_path (str): path to the sampled file\n",
        "    * sample_percent (float): percentage of dataset to be sampled\n",
        "    * chunksize (int): how many rows are to be read at a time\n",
        "  \"\"\"\n",
        "  # Count rows, samples\n",
        "  total_rows = 0\n",
        "  samples = []\n",
        "\n",
        "  # If zip file, unzip all files to the output directory and sample from each file\n",
        "  if check_zip(input_file_path):\n",
        "    output_dir = \"/\".join(output_file_path.split('/')[:-1])\n",
        "\n",
        "    for file_name in unzip(input_file_path, output_dir):\n",
        "      if not file_name:\n",
        "          continue\n",
        "\n",
        "      if not check_csv(file_name):\n",
        "        continue\n",
        "\n",
        "      print(f'Processing file: {file_name}')\n",
        "      rows, sp = chunk_file(file_name, sample_percent, chunksize)\n",
        "      total_rows += rows\n",
        "      samples.append(sp)\n",
        "\n",
        "  # If csv then sample then sample from csv\n",
        "  elif check_csv(input_file_path):\n",
        "      print(f'Processing file: {input_file_path}')\n",
        "      rows, sp = chunk_file(input_file_path, sample_percent, chunksize)\n",
        "      total_rows += rows\n",
        "      samples.append(sp)\n",
        "\n",
        "  # Check if samples exist\n",
        "  if samples:\n",
        "      sampled_df = pd.concat(samples)\n",
        "\n",
        "      # Summary print\n",
        "      print(f'Total Dataset Size: {total_rows}')\n",
        "      print(f'Expected Dataset Size: {total_rows*sample_percent}')\n",
        "      print(f'Sampled Dataset Size: {sampled_df.shape[0]}')\n",
        "\n",
        "      # Save sampled dataset to output file\n",
        "      sampled_df.to_csv(output_file_path, index=False)\n",
        "      return sampled_df\n",
        "\n",
        "  else:\n",
        "      print(\"No samples were extracted.\")\n",
        "      return None"
      ],
      "metadata": {
        "id": "KBuLHlY8pcWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage"
      ],
      "metadata": {
        "id": "2OU2Y5fb65Je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling a csv"
      ],
      "metadata": {
        "id": "bUNLqYExliZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File that you want sampled\n",
        "input_file_path =\"/content/drive/../input.csv\"\n",
        "\n",
        "# Path that you want the sampled dataset to be saved to\n",
        "output_file_path = \"/content/drive/../output.csv\"\n",
        "\n",
        "# Percentage of how much of the dataset you want to be sampled\n",
        "sample_percent = 0.01\n",
        "\n",
        "# Number of rows to be read at a time\n",
        "chunksize = 40000\n",
        "\n",
        "samples = random_sampler(input_file_path=input_file_path, output_file_path=output_file_path, sample_percent=sample_percent, chunksize=chunksize)"
      ],
      "metadata": {
        "id": "buzCeNBVYOx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling a zip"
      ],
      "metadata": {
        "id": "onmHSKTB2-mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# File that you want sampled\n",
        "input_file_path = \"/content/drive/../input.zip\"\n",
        "# Path that you want the sampled dataset to be saved to\n",
        "output_file_path = \"/content/drive/../output.zip\"\n",
        "\n",
        "# Percentage of how much of the dataset you want to be sampled\n",
        "sample_percent = 0.01\n",
        "\n",
        "# Number of rows to be read at a time\n",
        "chunksize = 10000\n",
        "\n",
        "\n",
        "samples = random_sampler(input_file_path=input_file_path, output_file_path=output_file_path, sample_percent=sample_percent, chunksize=chunksize)"
      ],
      "metadata": {
        "id": "_SModCjhh7Ny"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}